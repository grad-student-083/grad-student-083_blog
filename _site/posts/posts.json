[
  {
    "path": "posts/2022-23-10-Ferdous-faculty-apps/",
    "title": "Ferdous Faculty Applications: 2023",
    "description": "School specific application materials with deadlines.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2022-10-23",
    "categories": [],
    "contents": "\n\nNOTE: Most updated generic application\nmaterials (CV, research statement, teaching statement, diversity\nstatement) can be found here.\nSchool specific materials can be found below.\n\n\nUniversity\nTopic\nDeadline\nRemarks\nLink\nStatus\nMaterials\n1\nNC State (ISE)\n\nnext gen Mfg,\nrobotics \n Nov\n1/open\n-\nLink\nsubmitted\n\n2\nNC State (ME)\n\nnext gen Mfg,\nrobotics\nNov\n1/open\n-\nLink\nsubmitted\n\n3\nUT Austin\n(ME)\nMfg, Control,\nrobotics \nNov\n1\n-\nLink\nsubmitted\n\n4\nCarnegie Mellon\n(ME)\nControl,\nrobotics, AI/ML \nNov\n1\n-\nLink\nsubmitted\n\n5\nArizona State\n(ME)\nControl,\nAutonomous, robotics \nNov 5\n\n-\nLink \n\nsubmitted\n\n6\nSUNNY\nBinghamton (SSIE)\nAutonomous\nsystems \nrolling\n\n-\nLink\nsubmitted\n\n7\nCSUN (ME)\nAutonomous, robotics\nNov 17\n-\nLink\n\\(\\sim\\)\n\n8\nUniversity of Cincinnati (ISE)\nMfg, data analytics, Cyber-physical\nrolling\n-\nLink\n\\(\\sim\\)\n\n9\nNotre Dame (ME)\nRobotics, Mfg, Data Sci\nNov 4/Dec 1\n-\nLink\n\\(\\sim\\)\n\n10\nU Alabama (ME)\nAI/ML, robotics, Mfg\nrolling\n-\nLink\n\\(\\sim\\)\n\n11\nRensselaer (ME)\nAI/ML, robotics, Mfg\nrolling\n-\nLink\n\\(\\sim\\)\n\n12\nLouisiana State (ME)\nAI/ML, robotics, Mfg\nrolling\n-\nLink\n\\(\\sim\\)\n\n13\nStevens IT (ME)\nAI/ML, Design, robotics, Mfg\nrolling\n-\nLink\n\\(\\sim\\)\n\n14\nNorthwestern\n(ME) \nMfg, data\nanalytics, Mfg optiization \nNov 15\n\n-\nLink\nsubmitted\n\n15\nWichita State\n(ME) \nAI, robotics,\nMfg \nNov 15\n\n-\nLink\nsubmitted\n\n16\nOttawa (ME)\nCAN\nAI/ML,\nautonomous, robotics \nNov\n15\n-\nLink\nsubmitted\n\n17\nUni Dayton (ME)\nAI/ML, Design, Mfg\nNov 29\n-\nLink\n\\(\\sim\\)\n\n18\nArizona State (ME)\nMachine Leanring for ME\nNov 27\n-\nLink\n\\(\\sim\\)\n\n19\nUMich (ME)\nControl, Mfg\nDec 1\n-\nLink\n\\(\\sim\\)\n\n20\nOregon State (ME)\nMfg, Control, AI, robotics,\nmechatronics\nDec 1\n-\nLink\n\\(\\sim\\)\n\n21\nColumbia (ME)\nAutonomous systems, Desin and Mfg,\nRobotics\nDec 1\n-\nLink\n\\(\\sim\\)\n\n22\nTexas State (ME)\nAI/ML, Mfg\nDec 1\n-\nLink\n\\(\\sim\\)\n\n23\nEPFL (ME) Switzerland\nControl, Robotics, Mfg\nDec 1\n-\nLink\n\\(\\sim\\)\n\n24\nUC Merced (ME)\nAI/ML, Mfg\nDec 1\n-\nLink\n\\(\\sim\\)\n\n25\nOklahoma State (ME)\nAI/ML, robotics, Mfg\nDec 1\n-\nLink\n\\(\\sim\\)\n\n26\nSanta Clara (ME)\nDesign, Mfg, Robotics, Control\nDec 2\n-\nLink\n\\(\\sim\\)\n\n27\nUC Irvine (ME)\nAutonomous systems, Desin and Mfg,\nRobotics\nDec 11\n-\nLink\n\\(\\sim\\)\n\n28\nUC Berkeley (ME)\nAI design, ML, Mfg\nDec 12\n-\nLink\n\\(\\sim\\)\n\n29\nIllinois Tech (ME)\nAI/ML, robotics, Mfg\nDec 15\n-\nLink\n\\(\\sim\\)\n\n30\nNew Hapshire (ME)\nAI/ML, control/robotics, Mfg\nDec 15\n-\nLink\n\\(\\sim\\)\n\n31\nUni of Maryland (ME)\nAI/ML, manufacturing\nDec 15\n-\nLink\n\\(\\sim\\)\n\n32\nBoston University (ME)\nMfg, robotics, autonomous, data science,\ndesign\nDec 16\n-\nLink\n\\(\\sim\\)\n\n33\nUT Dallas (ME)\nDesign, Mfg, Automation, robotics\nOct 31/Dec 31\nJustin Koeln\nLink\n\\(\\sim\\)\n\n34\nRice (ME)\nDesign, Control, robotics\nDec 31\n-\nLink\n\\(\\sim\\)\n\n35\nBinghampton (ME)\nMfg, AI, autonomy\nDec 31\n-\nLink\n\\(\\sim\\)\n\n36\nSouthern_Illinois_carbondale (ME)\nControl, ML, robotics, Mfg\nJan 5\n-\nLink\n\\(\\sim\\)\n\n37\nUW Madison (ME)\nAutonomous systems, Mfg, Learning\nJan 12\n-\nLink\n\\(\\sim\\)\n\n38\nNJIT (MIE)\nAI/ML, design, Mfg, robotics\nrolling\n-\nLink\n\\(\\sim\\)\n\n39\nQueen’s (ECE) CAN\nAI/ML, autonomous, robotics\nJan 15\n-\nLink\n\\(\\sim\\)\n\n40\nQueen’s (ME) CAN\nAI/ML, autonomous, robotics\nJan 2\n-\nLink\n\\(\\sim\\)\n\nFerdous past\nresearch and future research plans:\n\nPostDoc applications:\n\nUniversity\nTopic\nDeadline\nRemarks\nLink\nStatus\nMaterials\n1\nMIT\n(ME)\nAI for\nengineering Design\nopen\nFaez\nAhmed\nLink\nsubmitted\n\n2\nUT Austin\n(ECE)\nRL, robotics\n\nopen\nAmy Zhang/Peter\nStone \nLink\nsubmitted\n\n3\nPNNL\nRL, Sci ML\n\nNov 8\n\n-\nLink\nsubmitted\n\n4\nArgonne NL\nML, optimal control\nopen\n-\nLink\n\\(\\sim\\)\n\n5\nCMU robotics\nrobotics, Deep learning\nNot specified\n-\nLink\n\\(\\sim\\)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-11-18T16:55:14-05:00",
    "input_file": "Ferdous-faculty-apps.knit.md"
  },
  {
    "path": "posts/2022-08-21-Transformer-implementation/",
    "title": "Transformer implementation deconstructed",
    "description": "A cheat sheet for implementing transformers.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2022-08-21",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nFormal\nalgorithm\nAttention\nEncoder\nDecoder\nBERT and GPT\n\n\nNOTE: Code implementation can be found at\nthis repo.\n\nIntroduction\nFew symbols:\nsymbol\ntensor shape\ndescription\n\\(B\\)\n-\nbatch size\n\\(l_z\\)\n-\ncontext/source sequence length\n\\(l_x\\)\n-\nprimary/target sequence length\n\\(d_\\text{model}\\)\n-\ninput dimension\n\\(d_k\\)\n-\ndimension of query/key embedding\n\\(d_v\\)\n-\ndimension of value embedding\n\\(B\\)\n-\nbatch size\n\\(h\\)\n-\nnumber of heads\n\\(LN\\)\n-\nlayer norm\n\\(FFN\\)\n-\nfeed forward network\n\\(\\mathbf{Z}\\)\n\\((B, l_z, d_\\text{model})\\)\ncontext/source sequence\n\\(\\mathbf{X}\\)\n\\((B, l_x, d_\\text{model})\\)\nprimary/target sequence\n\\(\\mathbf{M}_{\\mathbf{zz}}\\)\n\\((B, l_z, l_z)\\)\nsource mask\n\\(\\mathbf{M}_{\\mathbf{xx}}\\)\n\\((B, l_x, l_x)\\)\ntarget mask\n\\(\\mathbf{M}_{\\mathbf{xz}}\\)\n\\((l_z, l_z)\\)\nmemory mask\n\\(L_{enc}\\)\n-\nnumber of encoder layers\n\\(L_{dec}\\)\n-\nnumber of decoder layers\nWe start with the originally proposed encoder-decoder (ED)\ntransformer (Vaswani et al.\n2017). If we only use the encoder of the transformer, then it\nis similar to the BERT (Devlin et al. 2018) model and if we only\nuse the decoder of the transformer then it is similar to the GPT model\n(Radford et al.\n2018). For clarity, we consider a batched source sequence\ndata \\(\\mathbf{Z}\\) which consists of\n\\(B\\) sequences. Each sequence is of\nlength \\(l_z\\). This means that the\nsequence consists of \\(l_z\\) number of\ntokens or vector representation of some input.\n\\[ \\mathbf{Z} = \\begin{bmatrix}\n\\mathbf{z}^1_1 & \\mathbf{z}^1_2 & \\dots & \\mathbf{z}^1_{l_z}\n\\\\\n                \\mathbf{z}^2_1 & \\mathbf{z}^2_2 & \\dots &\n\\mathbf{z}^2_{l_z} \\\\\n                \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                \\mathbf{z}^{B}_1 & \\mathbf{z}^{B}_2 & \\dots\n& \\mathbf{z}^{B}_{l_z}\n                \\end{bmatrix}\\]\nwhere, \\[\\mathbf{z}_i \\in\n\\mathbb{R}^{1\\times d_\\text{model}}, \\ \\ \\ \\ \\ i = 1, 2, \\dots,\nl_z\\]\nSimilarly, we consider a batched target sequence data \\(\\mathbf{X}\\) that contain \\(B\\) sequences in total. Each target\nsequence is of length \\(l_x\\).\n\\[ \\mathbf{X} = \\begin{bmatrix}\n\\mathbf{x}^1_1 & \\mathbf{x}^1_2 & \\dots & \\mathbf{x}^1_{l_x}\n\\\\\n                \\mathbf{x}^2_1 & \\mathbf{x}^2_2 & \\dots &\n\\mathbf{x}^2_{l_x} \\\\\n                \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                \\mathbf{x}^{B}_1 & \\mathbf{x}^{B}_2 & \\dots\n& \\mathbf{x}^{B}_{l_x}\n                \\end{bmatrix}\\]\nwhere, \\[\\mathbf{x}_i \\in\n\\mathbb{R}^{1\\times d_\\text{model}}, \\ \\ \\ \\ \\ i = 1, 2, \\dots,\nl_x\\]\nThe goal is to learn a representation of the target sequence, \\(\\mathbf{X}\\), that utilizes multi-head\nattention to capture important correlation within the sequence. Finally,\npeople use these representations for various downstream tasks\ni.e. machine translation (Vaswani et al. 2017), next word\nprediction (Yang et al.\n2019), computer vision (Dosovitskiy et al. 2020), reinforcement\nlearning (Chen et al.\n2021) etc. A wonderful paper from DeepMind describes the\nformal algorithms for transformers (Phuong and Hutter 2022) very neatly.\nMask\nMask allows the transformer to decide which part of the output should\nthe model see at each timestep.\n\\[\\mathbf{M}_{\\mathbf{x}\\mathbf{z}} =\n\\begin{bmatrix} \\text{Mask}[\\mathbf{x}_0, \\mathbf{z}_0] &\n\\text{Mask}[\\mathbf{x}_0, \\mathbf{z}_1]&  \\text{Mask}[\\mathbf{x}_0,\n\\mathbf{z}_2] & \\dots &\\text{Mask}[\\mathbf{x}_0,\n\\mathbf{z}_{l_z}]  \\\\\n\\text{Mask}[\\mathbf{x}_1, \\mathbf{z}_0] & \\text{Mask}[\\mathbf{x}_1,\n\\mathbf{z}_1]&  \\text{Mask}[\\mathbf{x}_1, \\mathbf{z}_2] & \\dots\n& \\text{Mask}[\\mathbf{x}_1, \\mathbf{z}_{l_z}] \\\\\n\\vdots & \\vdots&  \\vdots & \\dots & \\vdots \\\\\n\\text{Mask}[\\mathbf{x}_{l_x}, \\mathbf{z}_0] &\n\\text{Mask}[\\mathbf{x}_{l_x},\n\\mathbf{z}_1]&  \\text{Mask}[\\mathbf{x}_{l_x}, \\mathbf{z}_2] &\n\\dots  & \\text{Mask}[\\mathbf{x}_{l_x}, \\mathbf{z}_{l_z}]\n\\end{bmatrix} \\in \\mathbb{R}^{l_x \\times l_z}\\]\nFor example, if we want the model to see the whole sequence to\ncalculate attention while training,then we do not need to do any\nmasking. This model deploys bidirectional attention.\nThe whole sequence should be available at the same time. For a single\n\\(i\\)-th sequence, the mask would look\nlike this \\[\\mathbf{M}_{\\mathbf{x}\\mathbf{z}}\n= \\begin{bmatrix} 1& 1&  1 & \\dots & 1  \\\\\n1& 1&  1 & \\dots & 1  \\\\\n\\vdots & \\vdots&  \\vdots & \\dots & \\vdots \\\\\n1& 1&  1 & \\dots & 1\n\\end{bmatrix} \\in \\mathbb{R}^{l_x \\times l_z}\\]\nSimilarly, for auto-regressive models, we want the model to calculate\nattention based on unseen outputs until that timestep. Hence, the mask\nwould be \\[\\mathbf{M}_{\\mathbf{x}\\mathbf{z}}\n= \\begin{bmatrix} 1& 0&  0 & \\dots & 0  \\\\\n1& 1&  0 & \\dots & 0  \\\\\n\\vdots & \\vdots&  \\vdots & \\dots & \\vdots \\\\\n1& 1&  1 & \\dots & 1\n\\end{bmatrix} \\in \\mathbb{R}^{l_x \\times l_z}\\]\nIn summary, if the length of each sequence at timestep \\(t\\) is \\(t_x\\) and \\(t_z\\) respectively, then we can express\neach mask as the following,\n\\[\\text{Mask}[t_x, t_z] = \\begin{cases} 1\n\\quad \\quad \\quad \\quad  \\text{for bidirectional attention} \\\\ [[t_x\n\\geq t_z]]  \\quad \\text{for unidirectional attention}\n\\end{cases}\\]\nFor conveniece, we introduce two additional terms:\nself-masks, \\(\\mathbf{M}_{\\mathbf{x}\\mathbf{x}}\\) or\n\\(\\mathbf{M}_{\\mathbf{z}\\mathbf{z}}\\):\nWhen we want to mask the same sequence against itself, for example we\nwould use this sort of masking in the encoder part\ncross-masks, \\(\\mathbf{M}_{\\mathbf{x}\\mathbf{z}}\\): When\nwe want to mask a target sequence against a source sequence, for example\nwe would use this sort of masking in the decoder part\nFormal algorithm\nFirst we provide a pseudoode of the encoder-decoder transformer\nalgorithm. The following pseudocode is a simplified version of the\nformal algorithm presented in this paper (Phuong and Hutter 2022). Initially I\nwanted to include the original pseudocode from the paper. But it seems\nlike handling a lot of notations while thinking of the implementation.\nSo, I added some trivial abstraction on top of that so the\nimplementation becomes more convenient. Also, each matrix in that paper\nis transposed which makes the batched implementation little bit\ndifficult to understand. So, I made some required modifications. This\nmay reduce the technical correctness of the pseudocode, but I think that\ncan be thought of as a simplification for implementation purpose.\nEncoder-decoder transformer\nAlgorithm 1: Encoder Decoder\nTransformer\ninput: \\(\\mathbf{Z} \\in \\mathbb{R}^{l_z \\times\nd_\\text{model}}\\), \\(\\mathbf{X} \\in\n\\mathbb{R}^{l_x \\times d_\\text{model} }\\), vector representations\nof context and primary sequence, \\(L_{enc}\\), \\(L_{dec}\\), number of encoder and decoder\nlayers, EncoderLayer class, DecoderLayer class, source_mask,\ntarget_mask, memory_mask\noutput: \\(\\mathbf{X} \\in \\mathbb{R}^{l_x \\times\nd_\\text{model}}\\), representation of primary sequence with\nmulti-head attention which can be used for downstream applications\n1 For \\(i = 1,\n2, \\dots, L_{enc}\\)\n2 \\(\\quad\n\\mathbf{Z} \\leftarrow \\text{EncoderLayer}(\\mathbf{Z},\n\\mathbf{M}_{\\mathbf{zz}}: Optional)\\)\n3 \\(\\text{memory} = \\mathbf{Z}\\)\n4 For \\(j = 1,\n2, \\dots, L_{dec}\\)\n5 \\(\\quad\n\\mathbf{X} \\leftarrow \\text{DecoderLayer}(\\mathbf{X}, \\text{memory},\n\\mathbf{M}_{\\mathbf{xx}}: Optional, \\mathbf{M}_{\\mathbf{zz}}: Optional,\n\\mathbf{M}_{\\mathbf{xz}}: Optional)\\)\n\nImplementation note: For efficiency,\ndeepcopy of a single encoder and decoder layer can be performed \\(L_{enc}\\) and \\(L_{dec}\\) times\n\nThe training procedure is fairly simple and basically same as other\nneural network models. To make a broaded sense, I am not including any\nNLP specific output from the model. So, the output from the model is\nwhat we are interested in. We will also need to supply the target so\nthat loss can be caluculated using the output and the target values.\nFinally we perform gradient descent to minimize the loss.\nTraining procedure\nAlgorithm 2: Training Transformer\n\\(\\text{input: }\n\\text{class EDTransformer, class loss_func, learning rate\n$\\eta$}\\)\n1 \\(\\text{for }\ni = 1, 2, \\dots, N_\\text{epochs}\\)\n2 \\(\\quad\n\\text{for } (\\mathbf{Z}, \\mathbf{X}, \\text{target}) \\text{ in\ntrain_dataloader } \\quad \\quad \\text{# typical data loader for training\ndata}\\)\n3 \\(\\quad \\quad\noutput \\leftarrow \\text{EDTransformer}(\\mathbf{Z}, \\mathbf{X},\n\\mathbf{M}_{\\mathbf{xx}}: Optional, \\mathbf{M}_{\\mathbf{zz}}: Optional,\n\\mathbf{M}_{\\mathbf{xz}}: Optional)\\)\n4 \\(\\quad \\quad\n\\mathcal{L}(\\mathbf{\\theta}) = \\text{loss_func}(\\text{output,\ntarget})\\)\n5 \\(\\quad \\quad\n\\theta \\leftarrow \\theta - \\eta \\cdot\n\\nabla\\mathcal{L}(\\theta)\\)\n6 \\(\\text{return\n} \\mathbf{\\theta}\\)\nAttention\nTransformers use multi-head attention to learn the contextual\ninformation of a sequence.\nMultihead attention\nThe original \\(Q, K, V\\) matrices\nare projected into \\(h\\) smaller\nmatrices of using parameter matrices \\(W_i^Q,\nW_i^K, W_i^V\\). Then attention is calculated for all these\nsmaller matrices and concatened again to calculate attention for the\nfull size input.\nargument\ntensor shape\nquery \\(Q\\)\n\\((B, l_x, d_\\text{model})\\)\nkey, \\(K\\)\n\\((B, l_z, d_\\text{model})\\)\nvalue, \\(V\\)\n\\((B, l_z, d_\\text{model})\\)\n\\(\\text{multi_attn}\\)\n\\((B, l_x, d_\\text{out})\\)\nLet’s recall the original definition of multi-head attention: \\[\\text{Multi-head attention, }\\mathbf{Y}(Q, K, V)\n= [\\mathbf{S}_1; \\dots; \\mathbf{S}_h]W^O\\] where \\(\\mathbf{S}_i\\) is the \\(i\\)-th single head attention score.\nAs we are dividing the original \\(Q, K,\nV\\) matrices into smaller matrices, dimension of \\(Q, K, V\\) must be divisible by the number\nof heads, \\(h\\). This is one way to do\nthat if we want to divide \\(Q, K, V\\)\ninto the same dimension of smaller matrices. \\[ d_k = d_v = d_\\text{model} / h\\]\nAlternatively, we can divide \\(Q, K,\nV\\) into different dimensions of smaller matrices as long as they\nmatch the original dimension.\nParameters\ndimension\nquery projection FFN, \\(W^Q\\)\n\\(\\mathbb{R}^{d_\\text{model}\\times\nd_k}\\)\nkey projection FFN, \\(W^K\\)\n\\(\\mathbb{R}^{d_\\text{model}\\times\nd_k}\\)\nvalue projection FFN, \\(W^V\\)\n\\(\\mathbb{R}^{d_\\text{out}\\times\nd_v}\\)\noutput projection FFN, \\(W^O\\)\n\\(\\mathbb{R}^{hd_v\\times\nd_\\text{out}}\\)\nSo, these parameter weight matrices help to project the original\n\\(Q, K, V\\) into smaller \\(h\\) number of \\(q, k, v\\) matrices for multi-head purpose.\n\\[\\text{query projection, }W_i^Q: Q\n\\rightarrow \\mathbf{q}_i\\] \\[\\text{key\nprojection, }W_i^K: K \\rightarrow \\mathbf{k}_i\\] \\[\\text{value projection, }W_i^V: V \\rightarrow\n\\mathbf{v}_i\\] For efficient implementation we calculate the\nattention score for all heads simultaneously by reshaping the tensors.\nSo, the shape of the smaller tensors end up being the following,\ntensor\nexpression\nshape\nefficient implementation\n\\(\\mathbf{q}_i\\)\n\\(QW^Q_i\\)\n\\((B, l_x, d_k)\\)\n\\((B, h, l_x, d_k)\\)\n\\(\\mathbf{k}_i\\)\n\\(KW^K_i\\)\n\\((B, l_z, d_k)\\)\n\\((B, h, l_z, d_k)\\)\n\\(\\mathbf{v}_i\\)\n\\(VW^V_i\\)\n\\((B, l_z, d_v)\\)\n\\((B, h, l_z, d_v)\\)\nNote that, for efficient implementation, we calculate \\(\\mathbf{q}_i, \\mathbf{k}_i, \\mathbf{v}_i\\)\nfor all heads simulatenously.\nScaled dot product attention\nNow, we can calculate attention score, not attention values, using\nthe originally proposed formula\n\\[\\mathbf{S}_i(Q, K, V) =\n\\text{softmax}\\left(\\frac{q_ik_i^T}{\\sqrt{d_k}}\\right)v_i\\] Next\nwe concatenate all the attention score to get the original dimension,\n\\[\\mathbf{S} \\leftarrow [\\mathbf{S}^1,\n\\mathbf{S}^2, \\dots, \\mathbf{S}^h]\\]\n\nImplementation note: For efficiency, we can\nimplement the heads simultaneously by reshapin the tensors, no need to\nconcat later either\n\nFinally, the attention values would be the following, \\[\\mathbf{W}^O: \\mathbf{S} \\rightarrow\n\\mathbf{Y}\\] Shape of the input and output tensors would be the\nfollwoing\ntensor\nshape\n\\(\\mathbf{S}\\)\n\\((B, l_x, h*d_v)\\)\n\\(\\mathbf{Y}\\)\n\\((B, l_x, d_\\text{out})\\)\nSelf attention,\ncross-attention\nDepending on how we create \\(Q, K,\nV\\) we can define two types of attention mechanism.\nself-attention: Same input, i.e. \\(\\mathbf{X}\\) or \\(\\mathbf{Z}\\) is used to represent all three\nmatrices, so \\[\\begin{matrix} \\mathbf{Q} =\n\\mathbf{X} \\\\ \\mathbf{K} = \\mathbf{X} \\\\ \\mathbf{V} = \\mathbf{X}\n\\end{matrix}\\]\ncross-attention: input, \\(\\mathbf{X}\\) is used to represent the\nquery, but output from another encoder, called \\(\\text{memory}\\), is used to represent key\nand value, so\\[\\begin{matrix} \\mathbf{Q} = \\mathbf{X} \\\\\n\\mathbf{K} = \\text{memory} \\\\ \\mathbf{V} = \\text{memory}\n\\end{matrix}\\]\nAlgorithm 3: Multihead attention\ninput: \\(\\mathbf{q} \\in \\mathbb{R}^{l_x \\times d_k},\n\\mathbf{k} \\in \\mathbb{R}^{l_z\\times d_k}, \\mathbf{v} \\in\n\\mathbb{R}^{l_z\\times d_v}, \\mathbf{M}_{\\mathbf{xz}} \\in \\{0,\n1\\}^{l_x\\times l_z}: \\text{Optional}\\)\n1 \\(\\mathbf{S}\n\\leftarrow \\mathbf{q}\\mathbf{k}^T\\)\n2 \\(\\mathbf{S}\n\\leftarrow\n\\text{softmax}\\left(\\frac{\\mathbf{S}}{d_k}\\right)\\)\n3 \\(\\mathbf{S}\n\\leftarrow \\mathbf{S}\\mathbf{v}\\)\n4 \\(\\mathbf{Y}\n\\leftarrow \\mathbf{W}^O \\mathbf{S}\\)\n\nImplementation note: For masking purpose,\nwe can replace each masked element in \\(\\mathbf{M}_\\mathbf{xz}\\) by \\(-\\infty\\) while making the non-masked\nelements as \\(0\\)s. In this way the\nsoftmax at line \\(2\\) makes the masked\nelement \\(0\\) while only keeping the\nnon-masked values\n\nEncoder\nEach encoder layer consists of two elements, 1) self-attention and 2)\nfeedforward network (FFN)\nAlgorithm 4: Encoder Layer\ninput: \\(\\quad\n\\mathbf{Z}: \\text{encoder input}, \\\\  \\quad \\text{class\nMultiheadAttention}, \\\\  \\quad \\mathbf{M}_{\\mathbf{zz}}:\n\\text{self-attention of encoder input}\\)\n1 \\(\\text{for }\nk = 1, 2, \\dots, h\\)\n2 \\(\\quad\n\\mathbf{Z} \\leftarrow \\mathbf{Z} +\n\\text{MultiheadAttention}(query=\\mathbf{Z}, key=\\mathbf{Z},\nvalue=\\mathbf{Z}, \\mathbf{M}_{\\mathbf{zz}}))\\)\n3 \\(\\quad\n\\mathbf{Z} \\leftarrow LN(\\mathbf{Z})\\)\n\nImplementation note: As we are implementing\nthe heads simultaneously, the loop is not really needed.\n\nDecoder\nEach decoder layer consists of three elements, 1) self-attention, 2)\ncross-attention and 3) feed forward network\nAlgorithm 5: Decoder layer\ninput: \\(\\quad\n\\mathbf{X}: \\text{decoder input}, \\\\  \\quad \\text{memory}: \\text{encoder\noutput},\\\\  \\quad \\text{class MultiheadAttention} \\\\  \\quad\n\\mathbf{M}_{\\mathbf{xx}}: \\text{self-attention of decoder\ninput},\\\\  \\quad \\mathbf{M}_{\\mathbf{xz}}: \\text{cross attention of\ndecoder input and encoder output}\\)\n2   \\(\\mathbf{X}\n= LN(\\mathbf{X} + \\text{MultiheadAttention}(query=\\mathbf{X},\nkey=\\mathbf{X}, value=\\mathbf{X},\n\\mathbf{M}_{\\mathbf{xx}})))\\)\n3   \\(\\mathbf{X}\n= LN(\\mathbf{X} + \\text{MultiheadAttention}(query=\\mathbf{X},\nkey=memory, value=memory, \\mathbf{M}_{\\mathbf{xz}}))\\)\n4   \\(\\mathbf{X}\n= LN(\\mathbf{X} + FFN(\\mathbf{X}))\\)\nBERT and GPT\nEncoder transformer (BERT)\nNow, we can define the BERT model in a very straightforward\nfashion.\nAlgorithm 6: Encoder Transformer\ninput: \\(\\quad\n\\mathbf{X}, \\text{ vector representations of primary sequence},\n\\\\  \\quad L_{enc}, \\text{ number of encoder layers, EncoderLayer class},\n\\\\  \\quad \\mathbf{M}_{\\mathbf{xx}}, \\text{ target mask}\\)\noutput: \\(\\quad\n\\mathbf{X}, \\text{ representation of primary sequence with multi-head\nattention} \\\\  \\text{ which can be used for downstream\napplications}\\)\n1 \\(\\text{for }\ni = 1, 2, \\dots, L_{enc}\\)\n2 \\(\\quad\n\\mathbf{X} \\leftarrow \\text{EncoderLayer}(\\mathbf{X},\n\\mathbf{M}_{\\mathbf{xx}}\\equiv 1)\\)\nSimilarly, the GPT model can be presented as the following\npseudocode.\nDecoder transformer (GPT)\nAlgorithm 7: Decoder Transformer\ninput: \\(\\quad\n\\mathbf{X}, \\text{vector representations of primary sequence}, \\\\  \\quad\nL_{dec}, \\text{number of decoder layers}, \\\\  \\quad \\text{class}\n\\text{  DecoderLayer}, \\\\  \\quad \\mathbf{M}_{\\mathbf{xx}}, \\text{target\nmask}\\)\noutput: \\(\\quad\n\\mathbf{X}, \\text{ representation of primary sequence with multi-head\nattention} \\\\ \\text{which can be used for downstream\napplications}\\)\n1 For \\(i = 1,\n2, \\dots, L_{dec}\\)\n2 \\(\\quad\n\\mathbf{X} \\leftarrow \\text{DecoderLayer}(\\mathbf{X},\n\\mathbf{M}_{\\mathbf{xx}}[t, t'] = [[t'\\geq t]])\\)\n\n\n\nChen, Lili, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover,\nMisha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021.\n“Decision Transformer: Reinforcement Learning via Sequence\nModeling.” Advances in Neural Information Processing\nSystems 34: 15084–97.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.\n“Bert: Pre-Training of Deep Bidirectional Transformers for\nLanguage Understanding.” arXiv Preprint\narXiv:1810.04805.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.\n2020. “An Image Is Worth 16x16 Words: Transformers for Image\nRecognition at Scale.” arXiv Preprint arXiv:2010.11929.\n\n\nPhuong, Mary, and Marcus Hutter. 2022. “Formal Algorithms for\nTransformers.” arXiv Preprint arXiv:2207.09238.\n\n\nRadford, Alec, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.\n2018. “Improving Language Understanding by Generative\nPre-Training.”\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” Advances in Neural\nInformation Processing Systems 30.\n\n\nYang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R\nSalakhutdinov, and Quoc V Le. 2019. “Xlnet: Generalized\nAutoregressive Pretraining for Language Understanding.”\nAdvances in Neural Information Processing Systems 32.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-26T14:25:49-04:00",
    "input_file": "Transformer-implementation.knit.md"
  },
  {
    "path": "posts/2021-09-25-RL-primer/",
    "title": "RL 0.0: Reinforcement learning primer",
    "description": "A trivial attempt to unify the fundamental RL concepts in one place for building intuitions.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-09-25",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nOnline sequential decision\nmaking\nBandit: A mandatory prior to\nRL\nMarkov Decision\nProcess: The RL formalism\nDynamic\nprogramming\nReinforcement\nlearning and optimal control\nBuilding algorithms for RL\nTypes of RL algorithms\nSample complexity of RL\nalgorithms\nA case study\nConclusion\n\nIntroduction\nHere I will try to explain how RL stems from the sequential decision\nmaking framework and its close relation with optimal control theory. I\nwill follow two primary references, reinforcement learning and optimal\ncontrol (Bertsekas\n2019) and introduction to reinforcement learning (Sutton and\nBarto 2018)\nOnline sequential decision\nmaking\nThe goal is to take sequential decisions “online” to achieve a\ncertain goal; often times it is maximizing a performance objective which\ncan be thougt of as a function \\(J(\\cdot)\\). The input to this objective\nfunction is not important right now. Let’s call this decision maker\n“agent”. The catch is that the agent has to figure out which decision to\ntake based on the observed feedback from the envrionment of its\ninterest. To observe a feedback the agent has to interact with the\nenvrionment through some sort of actions. So, optimization will be at\nthe core of this decision making procedure while we use data collected\nin an online fashion to identify actions to take. This is why the\n“learning” happens.\nBandit: A mandatory prior to\nRL\nConsider an online sequential decision making problem where an agent\nhas \\(k\\) choices to choose an action\nand everytime it executes an action it receives a feedback from the\nenvironment. A fundamental question then aries for the agent: how to\nchoose an action? The way it chooses an action describes its way of\nbehaving in this particular environment which is known as the “policy”\ndenoted as \\(\\pi\\). Note that \\(\\pi\\) decribes how to take an action but it\ndoes not say how to take the best action that will maximize the\nperformance objective \\(J(\\cdot)\\). To\nidentify the optimal action we need to find out the optimal policy \\(\\pi^*\\). So, the following makes sense\n\\[\\begin{equation}\n\\pi^* = \\text{argmax}_\\pi J^\\pi(\\cdot)\n\\end{equation}\\] where \\(J^\\pi(\\cdot)\\) is the value of the\nperformance objective obtained using policy \\(\\pi\\). Note that the agent does not know\nthe underlying distribution of the feedback from each action it takes.\nIf it were known then the agent could easily pick the best action. This\nsetting is known as the bandit problem or sometimes as\n“multi-armed bandit (MAB)” problem. Sometimes people call this “k-armed\nbandit” as well. Usually the feedback obtained from the environment is\nknown as reward or cost.\nMarkov Decision\nProcess: The RL formalism\nUntil now it is clear that we are interested in sequential decision\nmaking. To formalize such process we will adopt the `Markov\nDecision Process (MDP)’. An MDP \\(\\mathcal{M}\\) is usually expressed as a\ntuple of these following 5-elements. \\[\\mathcal{M} = \\langle \\mathcal{X}, \\mathcal{A},\n\\mathcal{R}, \\mathcal{P}, \\gamma\\rangle\\] where,\n\\(\\mathcal{X}\\) is the state-space,\na set of states, \\(\\mathbf{x}\\in\\mathcal{X}\\)\n\\(\\mathcal{A}\\) is the\naction-space, a set of actions, \\(a\\in\\mathcal{X}\\)\n\\(\\mathcal{R}\\) is the reward\nfunction, usually defined in the product space, \\(\\mathcal{R}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\n\\mathbb{R}\\)\n\\(\\mathcal{P}\\) is the transition\nprobability function, also known as the dynamics of the system, that\ndescribes the conditional probability \\(p(\\mathbf{x}_{t+1}|\\mathbf{x}_t,\na_t)\\)\n\\(\\gamma\\) is a discount factor,\n\\(\\gamma \\in [0, 1]\\)\nLet’s focus on how this formalism helps in sequential decision\nmaking. Assume that the agent is in a current state \\(\\mathbf{x}_t\\) at timestep \\(t\\). Based on some policy \\(\\pi\\) it takes a decision to move to state\n\\(\\mathbf{x}_{t+1}\\) by taking action\n\\(a_t\\). To move to that state, the\nagent needs to know the probability of moving to that state given the\ncurrent state \\(\\mathbf{x}_t\\) and\naction \\(a_t\\). This is how the\nconditional probability comes into the process. Once the agent reaches\nstate \\(\\mathbf{x}_{t+1}\\) it gets a\nfeedback from the environment. This feedback is called a reward value,\n\\(R_t\\), which is usually a scalar\nnumeric value. We assume that the reward value \\(R_t\\) comes as the output from the reward\nfunction \\(\\mathcal{R}\\) while it takes\n\\(\\mathbf{x}_t\\) and \\(a_t\\) as input, meaning \\(\\mathcal{R}(\\mathbf{x}_t, a_t): \\mathcal{X} \\times\n\\mathcal{A} \\rightarrow R_t\\). What if the reward function only\ndepends on the current state and not the action? Then the reward\nfunction would be represented as \\(\\mathcal{R}(\\mathbf{x}_t): \\mathcal{X} \\rightarrow\nR_t\\). Similary if the reward depends on not only the current\nstate and current action but also the state it ends up in, then we would\nuse the description of the reward function as \\(\\mathcal{R}(\\mathbf{x}_t, a_t, \\mathbf{x}_{t+1}):\n\\mathcal{X} \\times \\mathcal{A} \\times \\mathcal{X} \\rightarrow\nR_t\\). Finally the agent uses a discount factor \\(\\gamma\\) to put less weight onto future\nrewards and more weight into recent rewards. This makes sense because\nthe agent does not want to depend strongly on the information that comes\nafter many timesteps into the future. All these information can be\ncombined very convenienty in an MDP. Now it should be easier to follow\nwhy MDP is attractive for sequential decision making.\nThis formalism is great, but what is the goal of the agent in an MDP?\nIn simplified terms the ‘goal’ of the agent is to maximize the\naccumulation of rewards. Let’s define the accumulation of rewards as\nreturn. The return obtained at timestep \\(t\\) can be expressed as \\(G_t^\\pi = R_t + R_{t+1} + R_{t+2} +\n\\dots\\). This means that if the agent moves to state \\(\\mathbf{x}_{t+1}\\) from state \\(\\mathbf{x}_t\\) by taking action \\(a_t\\) it receives reward \\(R_t\\) and at the next timestep if it\nfollows the same policy it will receive reward \\(R_{t+1}\\) and so on. For an infinite\nhorizon case, the return will blow up. This is why we use a discount\nfactor \\(\\gamma\\) such that \\[ G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} +\n\\dots = \\sum_{t=0}^{\\infty} \\gamma^k R_{t+k}.\\]\nThis discount factor serves two purposes: a) it provides more weight\ninto recent rewards and b) it helps to keep the return as a finite\nvalue. If we do not use a discount factor in the MDP definition then\nthose MDPs are called ‘undiscounted MDPs’.\nBut how do we maximize the return? The answer is pretty simple: by\nchoosing the sequence of actions that provides the highest return. These\nactions are called ‘optimal actions’. Remember that actions are chosen\naccording to a policy \\(\\pi:\\mathcal{X}\n\\rightarrow \\mathcal{A}\\). So to choose the optimal actions we\nneed an optimal policy \\(\\pi^*\\). Now\nwe have successfully identified the fundamental goal in this learning\nscheme: “how to obtian the optimal policy for sequential decision\nmaking?”\nTo identify whether a state is good or bad we need to assign some\nsort of value to that state. Usually this is known as the value\nfunction. The agent would like to explore states which have\nhigher values compared to the rest. To derive the value of a state we\nuse the reward function in an intuitive way. Let’s take a look. \\[V^\\pi(\\mathbf{x}) =\n\\mathbb{E}^\\pi[G_t|\\mathbf{x}] = \\mathbb{E}^\\pi \\left[R_{t} +\n\\gamma  R_{t+1} + \\dots |\\mathbf{x}\\right] = \\mathbb{E}^\\pi\\left[\n\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k} |\\mathbf{x}\\right]\\] Here we\ntake the ‘expectation’ of the return to account for the stochasticity of\nthe rewards. Notice that we are only taking into consideration the mean\nvalue of the return, not the variance. This sometimes cause a variance\nissue in developed algorithms based on this formalism. So, what does\n\\(V^\\pi(\\mathbf{x})\\) mean? This means\nthat the value of a state while following a policy \\(\\pi\\) is the expected value of the return.\nLet’s develop a simple algorithm that can help us figure out the optimal\npolicy \\(\\pi^*\\) using the value of the\nstates.\nAlgorithm 1\n1 Find value of all states, \\(V^\\pi(\\mathbf{x})\\) where \\(\\mathbf{x}\\in\\mathcal{X}\\)\n2 From each state find the next best state\n\\(\\mathbf{x}_b =\n\\text{argmax}_{\\mathbf{x}'} V^\\pi(\\mathbf{x}')\\)\n3 Find the optimal policy by choosing the\naction that led to \\(\\mathbf{x}_b\\)\nmeaning \\(\\pi^*(\\mathbf{x}) = \\{a: \\mathbf{x}\n\\rightarrow \\mathbf{x}_b\\}\\)\nWould not it be better if we could, rather than finding the value of\na state, directly find the value of an action from a state? In that way\nwe would be able to evaluate whether an action is good or bad based on\nthe assigned value. Yes, we can and this is known as the action-value\nfunctions. These are also known as Q-values as they can\nbe informally thought of as the quality of an action taken from\na state. For convenience, an action taken from a state is\ncombinedly referred as the state-action, \\((\\mathbf{x}, a)\\), pair. So, how do we\ndefine Q-values? Looking closely to the definition of the\nvalue-functions we can similarly define the Q-values by conditioning the\nreturn on the state-action pair.\n\\[Q^\\pi(\\mathbf{x}, a) =\n\\mathbb{E}^\\pi[G_t| \\mathbf{x}, a] = \\mathbb{E}^\\pi \\left[R_{t} +\n\\gamma  R_{t+1} + \\dots |\\mathbf{x}, a\\right] = \\mathbb{E}^\\pi\\left[\n\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k}|\\mathbf{x}, a\\right]\\] So,\nQ-values are the values assigned to the state-action pair and values are\nassigned to the states only. Can we derive any relationship between them\nbased on their properties? To do that we need to break down their formal\ndefinition using the properties of the expectation operator in the above\nequations. Let’s break down the equation further using the definition of\nan expectation. We will use the following three properties of the\nexpectation operator.\n\n\nExpectation of a random variable\n\n\\(p1\\): Remember that if \\(X\\) is a discrete random variable with\nfinite number of outcomes \\(x_1, x_2, \\dots,\nx_k\\) with probabilities \\(p_1, p_2,\n\\dots, p_k\\) then \\[\\begin{equation}\n\\mathbb{E}[X] = p_1x_1 + p_2x_2 + \\dots +\np_kx_k\\end{equation}\\]\n\\(p2\\): Expectations are linear\noperator, meaning \\[\\mathbb{E}[X_1] +\n\\mathbb{E}[X_2] = \\mathbb{E}[X_1 + X_2]\\]\n\\(p3\\): For conditional\nexpectations using partition theorem, \\[\\mathbb{E}[X] = \\sum_y p(Y=y)\n\\mathbb{E}[X|Y=y]\\]\n\nSo, from the value function definition we get,\n\\[\\begin{aligned} V^\\pi(\\mathbf{x}) &=\n\\mathbb{E}^\\pi [R_t + \\gamma G_{t+1}|\\mathbf{x}] \\\\\n&= \\sum_{a\\in\\mathcal{A}} \\pi(a|\\mathbf{x}) \\mathbb{E}[R_t + \\gamma\nG_{t+1}|\\mathbf{x}, a] \\ \\ \\ \\ \\text{ using } p3\\\\\n&= \\sum_{a\\in\\mathcal{A}} \\pi(a|\\mathbf{x})\n\\sum_{\\mathbf{x}'\\in\\mathcal{\\mathcal{X}}} p(\\mathbf{x}, a,\n\\mathbf{x}') \\mathbb{E}[R_t + \\gamma G_{t+1}|\\mathbf{x}, a,\n\\mathbf{x}'] \\ \\ \\ \\ \\text{ using } p3\\\\\n&= \\sum_{a\\in\\mathcal{A}} \\pi(a|\\mathbf{x})\n\\sum_{\\mathbf{x}'\\in\\mathcal{\\mathcal{X}}} p(\\mathbf{x}, a,\n\\mathbf{x}') \\left[\\underbrace{\\mathbb{E}[R_t]}_{r(\\mathbf{x}, a)} +\n\\mathbb{E}[\\gamma G_{t+1}|\\mathbf{x}, a, \\mathbf{x}'] \\right] \\ \\ \\\n\\ \\text{ using } p2\\\\\n&= \\sum_{a\\in\\mathcal{A}} \\pi(a|\\mathbf{x})\n\\sum_{\\mathbf{x}'\\in\\mathcal{\\mathcal{X}}} p(\\mathbf{x}, a,\n\\mathbf{x}') \\left[ r(\\mathbf{x}, a) + \\gamma\n\\underbrace{\\mathbb{E}[G_{t+1}|\\mathbf{x}']}_{V^\\pi(\\mathbf{x}')}\n\\right] \\ \\ \\ \\ \\text{ using } p2\\\\\n&= \\sum_{a\\in\\mathcal{A}} \\pi(a|\\mathbf{x})\n\\sum_{\\mathbf{x}'\\in\\mathcal{\\mathcal{X}}} p(\\mathbf{x}, a,\n\\mathbf{x}') \\left[ r(\\mathbf{x}, a) + \\gamma V^\\pi(\\mathbf{x}')\n\\right]\n\\end{aligned}\\]\nThis gives us a recursive formula! Similarly we can formulate the\nQ-values.\n\\[\\begin{aligned}\nQ^\\pi(\\mathbf{x}, a) &= \\mathbb{E}^\\pi\\left[ R_t + \\gamma G_{t+1} |\n\\mathbf{x}, a\\right]\\\\\n&=  \\mathbb{E}^\\pi\\left[ R_t + \\gamma G_{t+1} | \\mathbf{x},\na\\right]\\\\\n&= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a, \\mathbf{x}')\n\\mathbb{E}^\\pi\\left[ R_t + \\gamma G_{t+1} | \\mathbf{x}, a,\n\\mathbf{x}'\\right]\\\\\n&= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a, \\mathbf{x}') \\left[\n\\mathbb{E}^\\pi[R_t|\\mathbf{x}, a, \\mathbf{x}'] + \\gamma\n\\mathbb{E}^\\pi \\left[G_{t+1} | \\mathbf{x}, a, \\mathbf{x}'\\right]\n\\right]\\\\\n&= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a, \\mathbf{x}') \\left[\nr(\\mathbf{x}, a) + \\gamma \\mathbb{E}^\\pi \\left[G_{t+1} |\n\\mathbf{x}'\\right] \\right]\\\\\n&= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a, \\mathbf{x}') \\left[\nr(\\mathbf{x}, a) + \\gamma V^\\pi(\\mathbf{x}')\\right]\n\\end{aligned}\\]\nWe can also develop relationship between the Q-values and value\nfunctions.\n\\[\\begin{aligned}\nV^\\pi(\\mathbf{x}) &= \\mathbb{E}^\\pi[G_t | \\mathbf{x}]\\\\\n&= \\sum_a \\pi(a|\\mathbf{x}) \\mathbb{E}^\\pi[G_t|\\mathbf{x}, a]\\\\\n&= \\sum_a \\pi(a|\\mathbf{x}) Q^\\pi(\\mathbf{x}, a)\n\\end{aligned}\\]\nFinally we develop the recursive formula for Q-values.\n\\[\\begin{aligned}\nQ^\\pi(\\mathbf{x}, a) &= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a,\n\\mathbf{x}') \\left[ r(\\mathbf{x}, a) + \\gamma\nV^\\pi(\\mathbf{x}')\\right]\\\\\n&= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a, \\mathbf{x}') \\left[\nr(\\mathbf{x}, a) + \\gamma \\sum_{a'} \\pi(a'|\\mathbf{x}')\nQ^\\pi(\\mathbf{x}', a') \\right]\n\\end{aligned}\\]\nIs there any way to determine the value of all the states within the\nstate-space? Yes, these algorithms are known as dynamic programming\nalgorithms.\nDynamic programming\nAlgorithm 2: Policy iteration\n1\n2\n3\n4\n5\nAlgorithm 2: Value iteration\n1\n2\n3\n4\n5\nNote that all the future rewards in the return equation is unknown.\nIf we knew about all the possible future rewards we are\ngoing to get from current timestep and forward, then we could obtain the\nexact value of the return. But unfortunately we do not have the luxury\nto know all these reward values apriori. This is where\nReinforcement Learning (RL) comes into play.\nReinforcement\nlearning and optimal control\nBuilding algorithms for RL\nTypes of RL algorithms\nSample complexity of RL\nalgorithms\nA case study\nConclusion\n\n\n\nBertsekas, Dimitri. 2019. Reinforcement and Optimal Control.\nAthena Scientific.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning:\nAn Introduction. MIT press.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-21T19:05:33-04:00",
    "input_file": "RL-primer.knit.md"
  },
  {
    "path": "posts/2021-04-19-rl-01-policy-gradient-methods/",
    "title": "RL 1.0: Policy gradient methods",
    "description": "Here we take a detailed view of policy gradient methods and their intuitions. This blog discuess how REINFORCE, baseline and actor-critic algorithms came into existence.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-04-19",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nPolicy based RL vs value based RL\nPolicy gradient theorem\nREINFORCE algorithm\nUsing baseline\nActor-critic algorithms\nCase study\nSummary\n\nIntroduction\nWe closely follow chapter 13 of the classic textbook of Sutton and Barto (2nd edition) (Sutton and Barto 2018). Initially we visit the classic policy gradient theorem and later build on top of that to develop REINFORCE and actor-critic algorithms. As usual our goal is to develop better intuition on how and why these algorithms work.\nPolicy based RL vs value based RL\nPolicy gradient theorem\nREINFORCE algorithm\nUsing baseline\nActor-critic algorithms\nCase study\nSummary\n\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT press.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-24T17:27:34-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-05-optimal-control-01-lqr/",
    "title": "Optimal Control 1.0: LQR",
    "description": "This post is part of a series of posts on optimal control theory. We take a detalied look on how classical LQR control is derived. A simple implementation is provided for clarity.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-04-05",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nNotation\nLinear system\n\nIntroduction\nLQR is an extremely popular optomal control framework. This blog closely follows (Duriez, Brunton, and Noack 2017).\nNotation\nSmall\nLinear system\nLet’s consider the linear system\n\\[\\begin{align} \n\\dot{\\mathbf{x}} &= \\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u}\\\\\n\\mathbf{y} &= \\mathbf{C}\\mathbf{x} + \\mathbf{D}\\mathbf{u}\\tag{1}\n\\end{align}\\]\nIf the system in (1) is controllable then a proportional controller can be designed as\n\\[\\begin{equation}\n\\mathbf{u} = -\\mathbf{K}_r \\mathbf{x}\\tag{2}\n\\end{equation}\\]\nHence the closed loop system becomes\n\\[\\begin{equation}\n\\dot{\\mathbf{x}} = (\\mathbf{A}-\\mathbf{B}\\mathbf{K}_r)\\mathbf{x}\\tag{3}\n\\end{equation}\\]\nWe can construct a quadratic cost \\(J\\) that balances the regulation of \\(\\mathbf{x}\\) with the cost of control input \\(\\mathbf{u}\\),\n\\[\\begin{equation}\nJ(t) = \\int_0^t [\\mathbf{x}^T(\\tau)\\mathbf{Q}\\mathbf{x}(\\tau)] + \\mathbf{u}^T(\\tau)\\mathbf{R}\\mathbf{u}(\\tau)]\\tag{4}\n\\end{equation}\\]\nBy solving Algebraic Riccati Equation (ARE) we get the optimal control law,\n\\[\\begin{equation}\n\\mathbf{K}_r = \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\tag{5}\n\\end{equation}\\]\nwhere the ARE is expressed as\n\\[\\begin{equation}\n\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} - \\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P} + \\mathbf{Q} = 0\\tag{6}\n\\end{equation}\\]\n\n\n\nDuriez, Thomas, Steven L Brunton, and Bernd R Noack. 2017. Machine Learning Control-Taming Nonlinear Dynamics and Turbulence. Springer.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-24T17:21:34-04:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to The Graduate Student Perspective",
    "description": {},
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-03-29",
    "categories": [],
    "contents": "\nWhy graduate student perspective?\nMachine learning, Reinforcement learning and control theory are vast! The stream of research articles are often overwhelming. Sometimes I like to take a step back and surprisingly I find peace exploring fundamental topics and explaining them. I remeber there was a time when I wanted a blog that explains complex topics in intuitive ways. Now-a-days the chaotic stream of online blogs have made it extremely difficult for me to find a reliable blog post quickly. I personally enjoy a nice balance between formal presentation of concepts (because they are robust!) followed by an interesting toy example for building intuitions. I like to call this approach Intuitive formalism or often during my PhD research presentations I use the term visual formalism. Unfortunately I can not find a lot of online blog posts/writings according to my taste. So I decided to make a blog for myself that gave birth to this site. In the unlikely case of you reading this I appreciate your time here. Cheers! :)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-24T17:20:46-04:00",
    "input_file": {}
  }
]
