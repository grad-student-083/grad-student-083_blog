[
  {
    "path": "posts/2021-04-19-rl-01-policy-gradient-methods/",
    "title": "RL 01: Policy gradient methods",
    "description": "Here we take a detailed view of policy gradient methods and their intuitions. This blog discuess how REINFORCE, baseline and actor-critic algorithms came into existence.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-04-19",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nPolicy based RL vs value based RL\nPolicy gradient theorem\nREINFORCE algorithm\nUsing baseline\nActor-critic algorithms\nCase study\nSummary\n\nIntroduction\nWe closely follow chapter 13 of the classic textbook of Sutton and Barto (2nd edition) (Sutton and Barto 2018). Initially we visit the classic policy gradient theorem and later build on top of that to develop REINFORCE and actor-critic algorithms. As usual our goal is to develop better intuition on how and why these algorithms work.\nPolicy based RL vs value based RL\nPolicy gradient theorem\nREINFORCE algorithm\nUsing baseline\nActor-critic algorithms\nCase study\nSummary\n\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT press.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-19T04:09:26-04:00",
    "input_file": "rl-01-policy-gradient-methods.utf8.md"
  },
  {
    "path": "posts/2021-04-05-optimal-control-01-lqr/",
    "title": "Optimal Control 01: LQR",
    "description": "This post is part of a series of posts on optimal control theory. We take a detalied look on how classical LQR control is derived. A simple implementation is provided for clarity.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-04-05",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nLinear system\n\nIntroduction\nLQR is an extremely popular optomal control framework. This blog closely follows (Duriez, Brunton, and Noack 2017).\nLinear system\nLetâ€™s consider the linear system\n\\[\\begin{align} \n\\dot{\\mathbf{x}} &= \\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u}\\\\\n\\mathbf{y} &= \\mathbf{C}\\mathbf{x} + \\mathbf{D}\\mathbf{u}\\tag{1}\n\\end{align}\\]\nIf the system in (1) is controllable then a proportional controller can be designed as\n\\[\\begin{equation}\n\\mathbf{u} = -\\mathbf{K}_r \\mathbf{x}\\tag{2}\n\\end{equation}\\]\nHence the closed loop system becomes\n\\[\\begin{equation}\n\\dot{\\mathbf{x}} = (\\mathbf{A}-\\mathbf{B}\\mathbf{K}_r)\\mathbf{x}\\tag{3}\n\\end{equation}\\]\nWe can construct a quadratic cost \\(J\\) that balances the regulation of \\(\\mathbf{x}\\) with the cost of control input \\(\\mathbf{u}\\),\n\\[\\begin{equation}\nJ(t) = \\int_0^t [\\mathbf{x}^T(\\tau)\\mathbf{Q}\\mathbf{x}(\\tau)] + \\mathbf{u}^T(\\tau)\\mathbf{R}\\mathbf{u}(\\tau)]\\tag{4}\n\\end{equation}\\]\nBy solving Algebraic Riccati Equation (ARE) we get the optimal control law,\n\\[\\begin{equation}\n\\mathbf{K}_r = \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\tag{5}\n\\end{equation}\\]\nwhere the ARE is expressed as\n\\[\\begin{equation}\n\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} - \\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P} + \\mathbf{Q} = 0\\tag{6}\n\\end{equation}\\]\n\n\n\nDuriez, Thomas, Steven L Brunton, and Bernd R Noack. 2017. Machine Learning Control-Taming Nonlinear Dynamics and Turbulence. Springer.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-05T04:44:24-04:00",
    "input_file": "optimal-control-01-lqr.utf8.md"
  },
  {
    "path": "posts/2021-03-29-deep-rl-01-dqn/",
    "title": "Deep RL 01: DQN",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-03-29",
    "categories": [],
    "contents": "\nThe graduate student perspective on DQN.\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-05T02:47:49-04:00",
    "input_file": "deep-rl-01-dqn.utf8.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to The Graduate Student Perspective",
    "description": "Welcome to our new blog, The Graduate Student Perspective. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-03-29",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-05T02:46:28-04:00",
    "input_file": "welcome.utf8.md"
  }
]
