[
  {
    "path": "posts/2021-09-25-RL-primer/",
    "title": "RL primer: Intuitive formalism",
    "description": "A trivial attempt to unify the fundamental RL concepts in one place for building intuitions.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-09-25",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nOnline sequential decision making\nBandit: A mandatory prior to RL\nMarkov Decision Process: The RL formalism\nDynamic programming\nReinforcement learning and optimal control\nBuilding algorithms for RL\nValue based RL\nPolicy gradient methods\nActor critic methods\nSample complexity of RL algorithms\nConclusion\n\nIntroduction\nOnline sequential decision making\nBandit: A mandatory prior to RL\nMarkov Decision Process: The RL formalism\nDynamic programming\nReinforcement learning and optimal control\nBuilding algorithms for RL\nValue based RL\nPolicy gradient methods\nActor critic methods\nSample complexity of RL algorithms\nConclusion\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-25T04:33:28-04:00",
    "input_file": "RL-primer.utf8.md"
  },
  {
    "path": "posts/2021-04-19-rl-01-policy-gradient-methods/",
    "title": "RL 01: Policy gradient methods",
    "description": "Here we take a detailed view of policy gradient methods and their intuitions. This blog discuess how REINFORCE, baseline and actor-critic algorithms came into existence.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-04-19",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nPolicy based RL vs value based RL\nPolicy gradient theorem\nREINFORCE algorithm\nUsing baseline\nActor-critic algorithms\nCase study\nSummary\n\nIntroduction\nWe closely follow chapter 13 of the classic textbook of Sutton and Barto (2nd edition) (Sutton and Barto 2018). Initially we visit the classic policy gradient theorem and later build on top of that to develop REINFORCE and actor-critic algorithms. As usual our goal is to develop better intuition on how and why these algorithms work.\nPolicy based RL vs value based RL\nPolicy gradient theorem\nREINFORCE algorithm\nUsing baseline\nActor-critic algorithms\nCase study\nSummary\n\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT press.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-25T03:44:28-04:00",
    "input_file": "rl-01-policy-gradient-methods.utf8.md"
  },
  {
    "path": "posts/2021-04-05-optimal-control-01-lqr/",
    "title": "Optimal Control 01: LQR",
    "description": "This post is part of a series of posts on optimal control theory. We take a detalied look on how classical LQR control is derived. A simple implementation is provided for clarity.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-04-05",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nLinear system\n\nIntroduction\nLQR is an extremely popular optomal control framework. This blog closely follows (Duriez, Brunton, and Noack 2017).\nLinear system\nLetâ€™s consider the linear system\n\\[\\begin{align} \n\\dot{\\mathbf{x}} &= \\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u}\\\\\n\\mathbf{y} &= \\mathbf{C}\\mathbf{x} + \\mathbf{D}\\mathbf{u}\\tag{1}\n\\end{align}\\]\nIf the system in (1) is controllable then a proportional controller can be designed as\n\\[\\begin{equation}\n\\mathbf{u} = -\\mathbf{K}_r \\mathbf{x}\\tag{2}\n\\end{equation}\\]\nHence the closed loop system becomes\n\\[\\begin{equation}\n\\dot{\\mathbf{x}} = (\\mathbf{A}-\\mathbf{B}\\mathbf{K}_r)\\mathbf{x}\\tag{3}\n\\end{equation}\\]\nWe can construct a quadratic cost \\(J\\) that balances the regulation of \\(\\mathbf{x}\\) with the cost of control input \\(\\mathbf{u}\\),\n\\[\\begin{equation}\nJ(t) = \\int_0^t [\\mathbf{x}^T(\\tau)\\mathbf{Q}\\mathbf{x}(\\tau)] + \\mathbf{u}^T(\\tau)\\mathbf{R}\\mathbf{u}(\\tau)]\\tag{4}\n\\end{equation}\\]\nBy solving Algebraic Riccati Equation (ARE) we get the optimal control law,\n\\[\\begin{equation}\n\\mathbf{K}_r = \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\tag{5}\n\\end{equation}\\]\nwhere the ARE is expressed as\n\\[\\begin{equation}\n\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} - \\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P} + \\mathbf{Q} = 0\\tag{6}\n\\end{equation}\\]\n\n\n\nDuriez, Thomas, Steven L Brunton, and Bernd R Noack. 2017. Machine Learning Control-Taming Nonlinear Dynamics and Turbulence. Springer.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-05T04:44:24-04:00",
    "input_file": "optimal-control-01-lqr.utf8.md"
  },
  {
    "path": "posts/2021-03-29-deep-rl-01-dqn/",
    "title": "Deep RL 01: DQN",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-03-29",
    "categories": [],
    "contents": "\nThe graduate student perspective on DQN.\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-05T02:47:49-04:00",
    "input_file": "deep-rl-01-dqn.utf8.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to The Graduate Student Perspective",
    "description": "Welcome to our new blog, The Graduate Student Perspective. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-03-29",
    "categories": [],
    "contents": "\nWhy graduate student perspective?\nReinforcement learning and control theory are vast! The stream of research articles are often overwhelming. Sometimes I like to take a step back and surprisingly I find peace exploring fundamental topics and explaining them. I remeber there was a time when I wanted a blog that explains complex topics in intuitive ways. Now-a-days the chaotic stream of online blogs have made it extremely difficult for me to find a reliable blog post quickly. I personally enjoy a nice balance between formal presentation of concepts (because they are robust!) followed by an interesting toy example for building intuitions. I like to call this approach Intuitive formalism or often during my PhD research presentations I use the term visual formalism. Unfortunately I can not find a lot of online blog posts/writing according to my taste. So I decided to make a blog for myself that gave birth to this site. In case if you are reading this I appreciate your time here. Cheers! :)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-25T04:11:15-04:00",
    "input_file": "welcome.utf8.md"
  }
]
