[
  {
    "path": "posts/2022-08-21-Transformer-implementation/",
    "title": "Deconstructed transformer implementation",
    "description": "A cheat sheet for implementing transformers.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2022-08-21",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nFormal\nalgorithm\nAttention\nEncoder\nDecoder\nFew remarks\n\nIntroduction\nFormal algorithm\nAlgorithm 1: Encoder Decoder\nTransformer\ninput: input \\(x\\)\n1 For \\(l = 1,\n2, \\dots, L_{enc}\\)\n2 \\(\\quad x\n\\leftarrow \\text{EncoderLayer}(x)\\)\n3 For \\(l = 1,\n2, \\dots, L_{dec}\\)\n4 \\(\\quad x\n\\leftarrow \\text{DecoderLayer}(x)\\)\nAttention\nScaled dot product attention\n\\[\\text{Attention}(Q, K, V) =\n\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\] Shapes of\n\\(Q, K, V\\) matrices:\nsymbol\ndescription\n\\(N\\)\nbatch size\n\\(L\\)\ntarget sequence length\n\\(S\\)\nsource sequence length\n\\(d_k\\)\ndimension of query/key embedding\n\\(d_v\\)\ndimension of value embedding\nmatrix\nshape\nquery, \\(Q\\)\n\\((N, L, d_k)\\)\nkey, \\(K\\)\n\\((N, S, d_k)\\)\nvalue, \\(V\\)\n\\((N, S, d_v)\\)\nSelf attention and\ncross-attention\nMultihead attention\n\nNOTE: model dimension must be divisible by\nthe number of heads, meaning in python \\[\nd_\\text{model} \\  \\% \\ h == 0 \\]\n\nMultihead attention: \\[\\text{MultiHead}(Q,\nK, V) = [\\text{head}_1, \\dots, \\text{head}_h]W^O\\] where \\[\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K,\nVW_i^V)\\]\nProjection weight matrix\ndimension\nquery projection, \\(W^Q\\)\n\\(\\mathbb{R}^{d_\\text{model}\\times\nd_k}\\)\nkey projection, \\(W^K\\)\n\\(\\mathbb{R}^{d_\\text{model}\\times\nd_k}\\)\nvalue projection, \\(W^V\\)\n\\(\\mathbb{R}^{d_\\text{model}\\times\nd_v}\\)\noutput projection, \\(W^V\\)\n\\(\\mathbb{R}^{hd_v\\times\nd_\\text{out}}\\)\nEncoder\nEncoder consists of multiple layers. Each layer consists of two\nelements, 1) self-attention and 2) feedforward network\nAlgorithm 2: Encoder Layer\ninput: encoder input \\(x\\)\n1   \\(x = x +\n\\text{pos_encoding}(x)\\)      # if needed\n2   For \\(k = 1,\n2, \\dots, h\\)\n3   \\(\\quad x =\nLN(x + \\text{self_attention}(query=x, key=x, value=x)))\\)      #\nif needed\nself-attention block\nFeed forward block\nDecoder\nDecoder consists of multiple layers. Each layer consists of three\nelements, 1) self-attention, 2) cross-attention and 3) feed forward\nnetwork\nAlgorithm 3: Decoder block\ninput: decoder input \\(x\\), encoder output \\(memory\\)\n1   \\(x = x +\n\\text{pos_encoding}(x)\\)      # if needed\n2   \\(x = LN(x +\n\\text{self_attention}(query=x, key=x, value=x)))\\)\n3   \\(x = LN(x +\n\\text{cross_attention}(query=x, key=memory, value=memory))\\)\n4   \\(x = LN(x +\nFFN(x))\\)\nFew remarks\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-22T00:46:05-04:00",
    "input_file": "Transformer-implementation.knit.md"
  },
  {
    "path": "posts/2021-09-25-RL-primer/",
    "title": "RL 0.0: Reinforcement learning primer",
    "description": "A trivial attempt to unify the fundamental RL concepts in one place for building intuitions.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-09-25",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nOnline sequential decision\nmaking\nBandit: A mandatory prior to\nRL\nMarkov Decision\nProcess: The RL formalism\nDynamic\nprogramming\nReinforcement\nlearning and optimal control\nBuilding algorithms for RL\nTypes of RL algorithms\nSample complexity of RL\nalgorithms\nA case study\nConclusion\n\nIntroduction\nHere I will try to explain how RL stems from the sequential decision\nmaking framework and its close relation with optimal control theory. I\nwill follow two primary references, reinforcement learning and optimal\ncontrol (Bertsekas\n2019) and introduction to reinforcement learning (Sutton and\nBarto 2018)\nOnline sequential decision\nmaking\nThe goal is to take sequential decisions “online” to achieve a\ncertain goal; often times it is maximizing a performance objective which\ncan be thougt of as a function \\(J(\\cdot)\\). The input to this objective\nfunction is not important right now. Let’s call this decision maker\n“agent”. The catch is that the agent has to figure out which decision to\ntake based on the observed feedback from the envrionment of its\ninterest. To observe a feedback the agent has to interact with the\nenvrionment through some sort of actions. So, optimization will be at\nthe core of this decision making procedure while we use data collected\nin an online fashion to identify actions to take. This is why the\n“learning” happens.\nBandit: A mandatory prior to\nRL\nConsider an online sequential decision making problem where an agent\nhas \\(k\\) choices to choose an action\nand everytime it executes an action it receives a feedback from the\nenvironment. A fundamental question then aries for the agent: how to\nchoose an action? The way it chooses an action describes its way of\nbehaving in this particular environment which is known as the “policy”\ndenoted as \\(\\pi\\). Note that \\(\\pi\\) decribes how to take an action but it\ndoes not say how to take the best action that will maximize the\nperformance objective \\(J(\\cdot)\\). To\nidentify the optimal action we need to find out the optimal policy \\(\\pi^*\\). So, the following makes sense\n\\[\\begin{equation}\n\\pi^* = \\text{argmax}_\\pi J^\\pi(\\cdot)\n\\end{equation}\\] where \\(J^\\pi(\\cdot)\\) is the value of the\nperformance objective obtained using policy \\(\\pi\\). Note that the agent does not know\nthe underlying distribution of the feedback from each action it takes.\nIf it were known then the agent could easily pick the best action. This\nsetting is known as the bandit problem or sometimes as\n“multi-armed bandit (MAB)” problem. Sometimes people call this “k-armed\nbandit” as well. Usually the feedback obtained from the environment is\nknown as reward or cost.\nMarkov Decision\nProcess: The RL formalism\nUntil now it is clear that we are interested in sequential decision\nmaking. To formalize such process we will adopt the `Markov\nDecision Process (MDP)’. An MDP \\(\\mathcal{M}\\) is usually expressed as a\ntuple of these following 5-elements. \\[\\mathcal{M} = \\langle \\mathcal{X}, \\mathcal{A},\n\\mathcal{R}, \\mathcal{P}, \\gamma\\rangle\\] where,\n\\(\\mathcal{X}\\) is the state-space,\na set of states, \\(\\mathbf{x}\\in\\mathcal{X}\\)\n\\(\\mathcal{A}\\) is the\naction-space, a set of actions, \\(a\\in\\mathcal{X}\\)\n\\(\\mathcal{R}\\) is the reward\nfunction, usually defined in the product space, \\(\\mathcal{R}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\n\\mathbb{R}\\)\n\\(\\mathcal{P}\\) is the transition\nprobability function, also known as the dynamics of the system, that\ndescribes the conditional probability \\(p(\\mathbf{x}_{t+1}|\\mathbf{x}_t,\na_t)\\)\n\\(\\gamma\\) is a discount factor,\n\\(\\gamma \\in [0, 1]\\)\nLet’s focus on how this formalism helps in sequential decision\nmaking. Assume that the agent is in a current state \\(\\mathbf{x}_t\\) at timestep \\(t\\). Based on some policy \\(\\pi\\) it takes a decision to move to state\n\\(\\mathbf{x}_{t+1}\\) by taking action\n\\(a_t\\). To move to that state, the\nagent needs to know the probability of moving to that state given the\ncurrent state \\(\\mathbf{x}_t\\) and\naction \\(a_t\\). This is how the\nconditional probability comes into the process. Once the agent reaches\nstate \\(\\mathbf{x}_{t+1}\\) it gets a\nfeedback from the environment. This feedback is called a reward value,\n\\(R_t\\), which is usually a scalar\nnumeric value. We assume that the reward value \\(R_t\\) comes as the output from the reward\nfunction \\(\\mathcal{R}\\) while it takes\n\\(\\mathbf{x}_t\\) and \\(a_t\\) as input, meaning \\(\\mathcal{R}(\\mathbf{x}_t, a_t): \\mathcal{X} \\times\n\\mathcal{A} \\rightarrow R_t\\). What if the reward function only\ndepends on the current state and not the action? Then the reward\nfunction would be represented as \\(\\mathcal{R}(\\mathbf{x}_t): \\mathcal{X} \\rightarrow\nR_t\\). Similary if the reward depends on not only the current\nstate and current action but also the state it ends up in, then we would\nuse the description of the reward function as \\(\\mathcal{R}(\\mathbf{x}_t, a_t, \\mathbf{x}_{t+1}):\n\\mathcal{X} \\times \\mathcal{A} \\times \\mathcal{X} \\rightarrow\nR_t\\). Finally the agent uses a discount factor \\(\\gamma\\) to put less weight onto future\nrewards and more weight into recent rewards. This makes sense because\nthe agent does not want to depend strongly on the information that comes\nafter many timesteps into the future. All these information can be\ncombined very convenienty in an MDP. Now it should be easier to follow\nwhy MDP is attractive for sequential decision making.\nThis formalism is great, but what is the goal of the agent in an MDP?\nIn simplified terms the ‘goal’ of the agent is to maximize the\naccumulation of rewards. Let’s define the accumulation of rewards as\nreturn. The return obtained at timestep \\(t\\) can be expressed as \\(G_t^\\pi = R_t + R_{t+1} + R_{t+2} +\n\\dots\\). This means that if the agent moves to state \\(\\mathbf{x}_{t+1}\\) from state \\(\\mathbf{x}_t\\) by taking action \\(a_t\\) it receives reward \\(R_t\\) and at the next timestep if it\nfollows the same policy it will receive reward \\(R_{t+1}\\) and so on. For an infinite\nhorizon case, the return will blow up. This is why we use a discount\nfactor \\(\\gamma\\) such that \\[ G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} +\n\\dots = \\sum_{t=0}^{\\infty} \\gamma^k R_{t+k}.\\]\nThis discount factor serves two purposes: a) it provides more weight\ninto recent rewards and b) it helps to keep the return as a finite\nvalue. If we do not use a discount factor in the MDP definition then\nthose MDPs are called ‘undiscounted MDPs’.\nBut how do we maximize the return? The answer is pretty simple: by\nchoosing the sequence of actions that provides the highest return. These\nactions are called ‘optimal actions’. Remember that actions are chosen\naccording to a policy \\(\\pi:\\mathcal{X}\n\\rightarrow \\mathcal{A}\\). So to choose the optimal actions we\nneed an optimal policy \\(\\pi^*\\). Now\nwe have successfully identified the fundamental goal in this learning\nscheme: “how to obtian the optimal policy for sequential decision\nmaking?”\nTo identify whether a state is good or bad we need to assign some\nsort of value to that state. Usually this is known as the value\nfunction. The agent would like to explore states which have\nhigher values compared to the rest. To derive the value of a state we\nuse the reward function in an intuitive way. Let’s take a look. \\[V^\\pi(\\mathbf{x}) =\n\\mathbb{E}^\\pi[G_t|\\mathbf{x}] = \\mathbb{E}^\\pi \\left[R_{t} +\n\\gamma  R_{t+1} + \\dots |\\mathbf{x}\\right] = \\mathbb{E}^\\pi\\left[\n\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k} |\\mathbf{x}\\right]\\] Here we\ntake the ‘expectation’ of the return to account for the stochasticity of\nthe rewards. Notice that we are only taking into consideration the mean\nvalue of the return, not the variance. This sometimes cause a variance\nissue in developed algorithms based on this formalism. So, what does\n\\(V^\\pi(\\mathbf{x})\\) mean? This means\nthat the value of a state while following a policy \\(\\pi\\) is the expected value of the return.\nLet’s develop a simple algorithm that can help us figure out the optimal\npolicy \\(\\pi^*\\) using the value of the\nstates.\nAlgorithm 1\n1 Find value of all states, \\(V^\\pi(\\mathbf{x})\\) where \\(\\mathbf{x}\\in\\mathcal{X}\\)\n2 From each state find the next best state\n\\(\\mathbf{x}_b =\n\\text{argmax}_{\\mathbf{x}'} V^\\pi(\\mathbf{x}')\\)\n3 Find the optimal policy by choosing the\naction that led to \\(\\mathbf{x}_b\\)\nmeaning \\(\\pi^*(\\mathbf{x}) = \\{a: \\mathbf{x}\n\\rightarrow \\mathbf{x}_b\\}\\)\nWould not it be better if we could, rather than finding the value of\na state, directly find the value of an action from a state? In that way\nwe would be able to evaluate whether an action is good or bad based on\nthe assigned value. Yes, we can and this is known as the action-value\nfunctions. These are also known as Q-values as they can\nbe informally thought of as the quality of an action taken from\na state. For convenience, an action taken from a state is\ncombinedly referred as the state-action, \\((\\mathbf{x}, a)\\), pair. So, how do we\ndefine Q-values? Looking closely to the definition of the\nvalue-functions we can similarly define the Q-values by conditioning the\nreturn on the state-action pair.\n\\[Q^\\pi(\\mathbf{x}, a) =\n\\mathbb{E}^\\pi[G_t| \\mathbf{x}, a] = \\mathbb{E}^\\pi \\left[R_{t} +\n\\gamma  R_{t+1} + \\dots |\\mathbf{x}, a\\right] = \\mathbb{E}^\\pi\\left[\n\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k}|\\mathbf{x}, a\\right]\\] So,\nQ-values are the values assigned to the state-action pair and values are\nassigned to the states only. Can we derive any relationship between them\nbased on their properties? To do that we need to break down their formal\ndefinition using the properties of the expectation operator in the above\nequations. Let’s break down the equation further using the definition of\nan expectation. We will use the following three properties of the\nexpectation operator.\n\n\nExpectation of a random variable\n\n\\(p1\\): Remember that if \\(X\\) is a discrete random variable with\nfinite number of outcomes \\(x_1, x_2, \\dots,\nx_k\\) with probabilities \\(p_1, p_2,\n\\dots, p_k\\) then \\[\\begin{equation}\n\\mathbb{E}[X] = p_1x_1 + p_2x_2 + \\dots +\np_kx_k\\end{equation}\\]\n\\(p2\\): Expectations are linear\noperator, meaning \\[\\mathbb{E}[X_1] +\n\\mathbb{E}[X_2] = \\mathbb{E}[X_1 + X_2]\\]\n\\(p3\\): For conditional\nexpectations using partition theorem, \\[\\mathbb{E}[X] = \\sum_y p(Y=y)\n\\mathbb{E}[X|Y=y]\\]\n\nSo, from the value function definition we get,\n\\[\\begin{aligned} V^\\pi(\\mathbf{x}) &=\n\\mathbb{E}^\\pi [R_t + \\gamma G_{t+1}|\\mathbf{x}] \\\\\n&= \\sum_{a\\in\\mathcal{A}} \\pi(a|\\mathbf{x}) \\mathbb{E}[R_t + \\gamma\nG_{t+1}|\\mathbf{x}, a] \\ \\ \\ \\ \\text{ using } p3\\\\\n&= \\sum_{a\\in\\mathcal{A}} \\pi(a|\\mathbf{x})\n\\sum_{\\mathbf{x}'\\in\\mathcal{\\mathcal{X}}} p(\\mathbf{x}, a,\n\\mathbf{x}') \\mathbb{E}[R_t + \\gamma G_{t+1}|\\mathbf{x}, a,\n\\mathbf{x}'] \\ \\ \\ \\ \\text{ using } p3\\\\\n&= \\sum_{a\\in\\mathcal{A}} \\pi(a|\\mathbf{x})\n\\sum_{\\mathbf{x}'\\in\\mathcal{\\mathcal{X}}} p(\\mathbf{x}, a,\n\\mathbf{x}') \\left[\\underbrace{\\mathbb{E}[R_t]}_{r(\\mathbf{x}, a)} +\n\\mathbb{E}[\\gamma G_{t+1}|\\mathbf{x}, a, \\mathbf{x}'] \\right] \\ \\ \\\n\\ \\text{ using } p2\\\\\n&= \\sum_{a\\in\\mathcal{A}} \\pi(a|\\mathbf{x})\n\\sum_{\\mathbf{x}'\\in\\mathcal{\\mathcal{X}}} p(\\mathbf{x}, a,\n\\mathbf{x}') \\left[ r(\\mathbf{x}, a) + \\gamma\n\\underbrace{\\mathbb{E}[G_{t+1}|\\mathbf{x}']}_{V^\\pi(\\mathbf{x}')}\n\\right] \\ \\ \\ \\ \\text{ using } p2\\\\\n&= \\sum_{a\\in\\mathcal{A}} \\pi(a|\\mathbf{x})\n\\sum_{\\mathbf{x}'\\in\\mathcal{\\mathcal{X}}} p(\\mathbf{x}, a,\n\\mathbf{x}') \\left[ r(\\mathbf{x}, a) + \\gamma V^\\pi(\\mathbf{x}')\n\\right]\n\\end{aligned}\\]\nThis gives us a recursive formula! Similarly we can formulate the\nQ-values.\n\\[\\begin{aligned}\nQ^\\pi(\\mathbf{x}, a) &= \\mathbb{E}^\\pi\\left[ R_t + \\gamma G_{t+1} |\n\\mathbf{x}, a\\right]\\\\\n&=  \\mathbb{E}^\\pi\\left[ R_t + \\gamma G_{t+1} | \\mathbf{x},\na\\right]\\\\\n&= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a, \\mathbf{x}')\n\\mathbb{E}^\\pi\\left[ R_t + \\gamma G_{t+1} | \\mathbf{x}, a,\n\\mathbf{x}'\\right]\\\\\n&= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a, \\mathbf{x}') \\left[\n\\mathbb{E}^\\pi[R_t|\\mathbf{x}, a, \\mathbf{x}'] + \\gamma\n\\mathbb{E}^\\pi \\left[G_{t+1} | \\mathbf{x}, a, \\mathbf{x}'\\right]\n\\right]\\\\\n&= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a, \\mathbf{x}') \\left[\nr(\\mathbf{x}, a) + \\gamma \\mathbb{E}^\\pi \\left[G_{t+1} |\n\\mathbf{x}'\\right] \\right]\\\\\n&= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a, \\mathbf{x}') \\left[\nr(\\mathbf{x}, a) + \\gamma V^\\pi(\\mathbf{x}')\\right]\n\\end{aligned}\\]\nWe can also develop relationship between the Q-values and value\nfunctions.\n\\[\\begin{aligned}\nV^\\pi(\\mathbf{x}) &= \\mathbb{E}^\\pi[G_t | \\mathbf{x}]\\\\\n&= \\sum_a \\pi(a|\\mathbf{x}) \\mathbb{E}^\\pi[G_t|\\mathbf{x}, a]\\\\\n&= \\sum_a \\pi(a|\\mathbf{x}) Q^\\pi(\\mathbf{x}, a)\n\\end{aligned}\\]\nFinally we develop the recursive formula for Q-values.\n\\[\\begin{aligned}\nQ^\\pi(\\mathbf{x}, a) &= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a,\n\\mathbf{x}') \\left[ r(\\mathbf{x}, a) + \\gamma\nV^\\pi(\\mathbf{x}')\\right]\\\\\n&= \\sum_{\\mathbf{x}'}p(\\mathbf{x}, a, \\mathbf{x}') \\left[\nr(\\mathbf{x}, a) + \\gamma \\sum_{a'} \\pi(a'|\\mathbf{x}')\nQ^\\pi(\\mathbf{x}', a') \\right]\n\\end{aligned}\\]\nIs there any way to determine the value of all the states within the\nstate-space? Yes, these algorithms are known as dynamic programming\nalgorithms.\nDynamic programming\nAlgorithm 2: Policy iteration\n1\n2\n3\n4\n5\nAlgorithm 2: Value iteration\n1\n2\n3\n4\n5\nNote that all the future rewards in the return equation is unknown.\nIf we knew about all the possible future rewards we are\ngoing to get from current timestep and forward, then we could obtain the\nexact value of the return. But unfortunately we do not have the luxury\nto know all these reward values apriori. This is where\nReinforcement Learning (RL) comes into play.\nReinforcement\nlearning and optimal control\nBuilding algorithms for RL\nTypes of RL algorithms\nSample complexity of RL\nalgorithms\nA case study\nConclusion\n\n\n\nBertsekas, Dimitri. 2019. Reinforcement and Optimal Control.\nAthena Scientific.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning:\nAn Introduction. MIT press.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-21T19:05:33-04:00",
    "input_file": "RL-primer.knit.md"
  },
  {
    "path": "posts/2021-04-19-rl-01-policy-gradient-methods/",
    "title": "RL 1.0: Policy gradient methods",
    "description": "Here we take a detailed view of policy gradient methods and their intuitions. This blog discuess how REINFORCE, baseline and actor-critic algorithms came into existence.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-04-19",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nPolicy based RL vs value based RL\nPolicy gradient theorem\nREINFORCE algorithm\nUsing baseline\nActor-critic algorithms\nCase study\nSummary\n\nIntroduction\nWe closely follow chapter 13 of the classic textbook of Sutton and Barto (2nd edition) (Sutton and Barto 2018). Initially we visit the classic policy gradient theorem and later build on top of that to develop REINFORCE and actor-critic algorithms. As usual our goal is to develop better intuition on how and why these algorithms work.\nPolicy based RL vs value based RL\nPolicy gradient theorem\nREINFORCE algorithm\nUsing baseline\nActor-critic algorithms\nCase study\nSummary\n\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT press.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-24T17:27:34-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-05-optimal-control-01-lqr/",
    "title": "Optimal Control 1.0: LQR",
    "description": "This post is part of a series of posts on optimal control theory. We take a detalied look on how classical LQR control is derived. A simple implementation is provided for clarity.",
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-04-05",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nNotation\nLinear system\n\nIntroduction\nLQR is an extremely popular optomal control framework. This blog closely follows (Duriez, Brunton, and Noack 2017).\nNotation\nSmall\nLinear system\nLet’s consider the linear system\n\\[\\begin{align} \n\\dot{\\mathbf{x}} &= \\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u}\\\\\n\\mathbf{y} &= \\mathbf{C}\\mathbf{x} + \\mathbf{D}\\mathbf{u}\\tag{1}\n\\end{align}\\]\nIf the system in (1) is controllable then a proportional controller can be designed as\n\\[\\begin{equation}\n\\mathbf{u} = -\\mathbf{K}_r \\mathbf{x}\\tag{2}\n\\end{equation}\\]\nHence the closed loop system becomes\n\\[\\begin{equation}\n\\dot{\\mathbf{x}} = (\\mathbf{A}-\\mathbf{B}\\mathbf{K}_r)\\mathbf{x}\\tag{3}\n\\end{equation}\\]\nWe can construct a quadratic cost \\(J\\) that balances the regulation of \\(\\mathbf{x}\\) with the cost of control input \\(\\mathbf{u}\\),\n\\[\\begin{equation}\nJ(t) = \\int_0^t [\\mathbf{x}^T(\\tau)\\mathbf{Q}\\mathbf{x}(\\tau)] + \\mathbf{u}^T(\\tau)\\mathbf{R}\\mathbf{u}(\\tau)]\\tag{4}\n\\end{equation}\\]\nBy solving Algebraic Riccati Equation (ARE) we get the optimal control law,\n\\[\\begin{equation}\n\\mathbf{K}_r = \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\tag{5}\n\\end{equation}\\]\nwhere the ARE is expressed as\n\\[\\begin{equation}\n\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} - \\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P} + \\mathbf{Q} = 0\\tag{6}\n\\end{equation}\\]\n\n\n\nDuriez, Thomas, Steven L Brunton, and Bernd R Noack. 2017. Machine Learning Control-Taming Nonlinear Dynamics and Turbulence. Springer.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-24T17:21:34-04:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to The Graduate Student Perspective",
    "description": {},
    "author": [
      {
        "name": "Md Ferdous Alam",
        "url": "https://ferdous-alam.github.io"
      }
    ],
    "date": "2021-03-29",
    "categories": [],
    "contents": "\nWhy graduate student perspective?\nMachine learning, Reinforcement learning and control theory are vast! The stream of research articles are often overwhelming. Sometimes I like to take a step back and surprisingly I find peace exploring fundamental topics and explaining them. I remeber there was a time when I wanted a blog that explains complex topics in intuitive ways. Now-a-days the chaotic stream of online blogs have made it extremely difficult for me to find a reliable blog post quickly. I personally enjoy a nice balance between formal presentation of concepts (because they are robust!) followed by an interesting toy example for building intuitions. I like to call this approach Intuitive formalism or often during my PhD research presentations I use the term visual formalism. Unfortunately I can not find a lot of online blog posts/writings according to my taste. So I decided to make a blog for myself that gave birth to this site. In the unlikely case of you reading this I appreciate your time here. Cheers! :)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-24T17:20:46-04:00",
    "input_file": {}
  }
]
